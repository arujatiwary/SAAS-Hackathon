{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoMP6_UqMzos",
        "outputId": "3f754ed7-c729-43e1-b402-b066aecaee49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "label      0\n",
            "dtype: int64\n",
            "label\n",
            "0    23481\n",
            "1    21417\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "real_news = pd.read_csv(\"True.csv\")\n",
        "fake_news = pd.read_csv(\"Fake.csv\")\n",
        "\n",
        "# Add labels\n",
        "real_news['label'] = 1  # 1 for real news\n",
        "fake_news['label'] = 0  # 0 for fake news\n",
        "\n",
        "# Combine datasets\n",
        "dataset = pd.concat([real_news, fake_news], ignore_index=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Check class distribution\n",
        "print(dataset['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC4xBB9QSVyt",
        "outputId": "8aa3a86d-c879-43fd-8d8c-31e945e9f006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0  As U.S. budget fight looms, Republicans flip t...   \n",
            "1  U.S. military to accept transgender recruits o...   \n",
            "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
            "3  FBI Russia probe helped by Australian diplomat...   \n",
            "4  Trump wants Postal Service to charge 'much mor...   \n",
            "\n",
            "                                                text       subject  \\\n",
            "0  washington reuters head conservative republica...  politicsNews   \n",
            "1  washington reuters transgender people allowed ...  politicsNews   \n",
            "2  washington reuters special counsel investigati...  politicsNews   \n",
            "3  washington reuters trump campaign adviser geor...  politicsNews   \n",
            "4  seattlewashington reuters president donald tru...  politicsNews   \n",
            "\n",
            "                 date  label  \n",
            "0  December 31, 2017       1  \n",
            "1  December 29, 2017       1  \n",
            "2  December 31, 2017       1  \n",
            "3  December 30, 2017       1  \n",
            "4  December 29, 2017       1  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (only need to run once)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Download the punkt_tab dataset\n",
        "nltk.download('punkt_tab') # This line was added to fix the LookupError\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a single string\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'text' column in the dataset\n",
        "dataset['text'] = dataset['text'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows of the preprocessed dataset\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MgF2kK9NT2MW"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(dataset['text']).toarray()\n",
        "y = dataset['label']\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmS8H_emSwFe",
        "outputId": "9916a8c2-2ac2-4612-835e-e6e3b06959b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for Random Forest:\n",
            "Accuracy: 0.9969933184855234\n",
            "Precision: 0.9960802397970948\n",
            "Recall: 0.9976905311778291\n",
            "F1-Score: 0.9968847352024922\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"Evaluation for {model_name}:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred)}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred)}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred)}\")\n",
        "\n",
        "evaluate_model(y_test, y_pred_rf, \"Random Forest\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}